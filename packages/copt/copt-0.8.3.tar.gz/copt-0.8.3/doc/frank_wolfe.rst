.. _frank_wolfe:

Frank-Wolfe
===========

This section describes the implementation of the Frank-Wolfe and other projection-free algorithms


Frank-Wolfe
-----------

The Frank-Wolfe (FW) or conditional gradient algorithm [J2003]_, [P2018]_, [PANJ2018]_ is a method for constrained optimization. It can solve problems of the form

.. math::
      \argmin_{\bs{x} \in \mathcal{D}} f(\bs{x})

where :math:`f` is a differentiable function for which we have access to its gradient and :math:`\mathcal{D}` is a compact set for which we have access to its linear minimization oracle (lmo). This is a routine that given a vector :math:`\bs{u}` returns a solution to

.. math::
    \argmin_{\bs{x} \in D}\, \langle\bs{u}, \bs{x}\rangle~.


Contrary to other constrained optimization algorithms like projected gradient descent, the Frank-Wolfe algorithm does not require access to a projection, hence why it is sometimes referred to as a projection-free algorithm. It instead relies exclusively on the linear minimization oracle described above.


.. TODO describe the LMO API in more detail


The Frank-Wolfe algorithm is implemented in this library in the method :meth:`copt.minimize_frank_wolfe`. As most other methods it takes as argument an objective function to minimize, but unlike most other methods, it requires access to a *linear minimization oracle*, which is a routine that for a given $d$-dimensional vector :math:`\bs{u}` solves the linear problems  :math:`\argmin_{\bs{z} \in D}\, \langle \nabla u, \bs{z}\rangle`.


At each iteration, the Frank-Wolfe algorithm uses the linear minimization oracle to identify the vertex :math:`\bs{s}_t`monitor that correlates most with the negative gradient. Then next iterate :math:`\boldsymbol{x}^+` is constructed as a convex combination of the current iterate :math:`\boldsymbol{x}` and the newly acquired vertex :math:`\boldsymbol{s}`:


.. math::
      \boldsymbol{x}^+ = (1 - \gamma)\boldsymbol{x} + \gamma \boldsymbol{s}



The step-size :math:`\gamma` can be chosen by different strategies:

  * **Inexact line-search**. This is the default option and corresponds to the keyword argument :code:`step_size="adaptive"` This is typically the fastest and simplest method, if unsure, use this option.

  * **Demyanov-Rubinov step-size**. This is a step-size of the form
    
    .. math::
        \gamma = \langle \nabla f(\bs{x}), \bs{s} - \bs{x}\rangle / (L \|\bs{s} - \bs{x}\|^2)~.



    This step-size typically performs well but has the drawback that it requires knowledge of the Lipschitz constant of :math:`\nabla f`. This step-size can be used with the keyword argument :code:`step_size="DR"`. In this case the Lipschitz
    constant :math:`L` needs to be specified through the keyword argument :code:`lipschitz`. For example, if the lipschitz constant is 0.1, then the signature should include :code:`step_size="DR", lipschitz=0.1`.


  * **Oblivious step-size**. This is the very simple step-size of the form
  
    .. math::
      \gamma = \frac{2}{t+2}~,
    
    where :math:`t` is the number of iterations. This step-size is oblivious since it doesn't use any previous information of the objective. It typically performs worst than the alternatives, but is simple to implement and can be competitive in the case in the case of noisy objectives.


Below is an illustration of the iterates generated by the Frank-Wolfe algorithkm on a toy 2-dimensional problem, in which the triangle is the domain  :math:`\mathcal{D}` and the level curves represent values of the objective function  :math:`f`.

.. image:: http://fa.bianp.net/images/2018/FW_iterates.png
  :alt: FW iterates
  :align: center



.. autosummary::
  :toctree: generated/

    copt.minimize_frank_wolfe


.. admonition:: Examples

   * :ref:`sphx_glr_auto_examples_frank_wolfe_plot_sparse_benchmark.py`
   * :ref:`sphx_glr_auto_examples_frank_wolfe_plot_vertex_overlap.py`



.. topic:: References:

  .. [J2003] Jaggi, Martin. `"Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization." <http://proceedings.mlr.press/v28/jaggi13-supp.pdf>`_ ICML 2013.

  .. [P2018] Pedregosa, Fabian `"Notes on the Frank-Wolfe Algorithm" <http://fa.bianp.net/blog/2018/notes-on-the-frank-wolfe-algorithm-part-i/>`_, 2018

  .. [PANJ2018] Pedregosa, Fabian, Armin Askari, Geoffrey Negiar, and Martin Jaggi. `"Step-Size Adaptivity in Projection-Free Optimization." <https://arxiv.org/pdf/1806.05123.pdf>`_ arXiv:1806.05123 (2018).



Pairwise Frank-Wolfe
--------------------

As the Frank-Wolfe algorithm, the Pairwise Frank-Wolfe [LJ2015]_ solves problems of the form 

.. math::
      \argmin_{\bs{x} \in \mathcal{D}} f(\bs{x})

where :math:`f`. is differentiable and the domain :math:`\mathcal{D}` is a convex and compart set.

Although the algorithm is more broadly applicable, this library's implementation, :meth:`copt.minimize_pairwise_frank_wolfe`, assumes that the domain :math:`\mathcal{D}` is the :math:`\ell_1` ball, that is, :math:`\mathcal{D} = \{x : \sum_i |x| \leq \alpha\}`, where :math:`\alpha` is a user-defined parameter.


.. autosummary::
   :toctree: generated/

    copt.minimize_pairwise_frank_wolfe


.. admonition:: Examples

  * :ref:`sphx_glr_auto_examples_frank_wolfe_plot_sparse_benchmark_pairwise.py`

.. topic:: References:

  .. [LJ2015] Lacoste-Julien, Simon, and Martin Jaggi. `"On the global linear convergence of Frank-Wolfe optimization variants." <https://arxiv.org/pdf/1511.05932.pdf>`_ Advances in Neural Information Processing Systems. 2015.
